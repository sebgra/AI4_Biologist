{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Konstvv/iBio-Summer-School/blob/main/iBio_Summer_school_PPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er_Q7bFjUnRm"
      },
      "source": [
        "# Protein-Protein Interaction prediction using Keras\n",
        "\n",
        "by Konstantin Volzhenin\n",
        "\n",
        "27/07/2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYTLgsfHU767"
      },
      "source": [
        "# Contents\n",
        "- Introduction\n",
        "- Dataset\n",
        "- Protein representations and embeddings\n",
        "- PPI prediction model\n",
        "- Data Analysis\n",
        "- C1 - C2 - C3 classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJAJ_0KHWCgS"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q9bC5kC-cmb"
      },
      "source": [
        "## 1. PPI Networks\n",
        "\n",
        "**Protein–protein interactions (PPIs)** are physical contacts established between two or more protein molecules as a result of biochemical events steered by various physical interactions. When studying a given organism (or an ecosystem with multiple organisms) the PPIs are usually combined together to form a protein-protein interaction network. Such networks or interactomes are of a great interest because of their value in different areas of protein related research (e.g. search for potential protein targets in therapeutic interest).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4898894/bin/npjschz201612-f2.jpg\" />\n",
        "</p>\n",
        "\n",
        "<center>\n",
        "Fig. 1 Schizophrenia interactome\n",
        "</center>\n",
        "    \n",
        "    Image taken from: Ganapathiraju MK, Thahir M, Handen A, Sarkar SN, Sweet RA, Nimgaonkar VL, Loscher CE, Bauer EM, Chaparala S (April 2016). \"Schizophrenia interactome with 504 novel protein-protein interactions\". NPJ Schizophrenia. 2: 16012.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 2. Protein-protein interactions task - binary classification\n",
        "\n",
        "During this class we will create a Deep Learning architechture for PPI prediction. The task itself is a binary classification: we have two sequences as an input and the model would have to estimate the probability of these two proteins to interact (a number between 0 and 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-rq5Jc-YJwc"
      },
      "source": [
        "# Dataset\n",
        "During this class we will work with PPI data\n",
        "of yeast *Saccharomyces cerevisiae*. An\n",
        "independent data set of 11 474 yeast PPIs contains approximately equal number of interacting and non-interacting pairs.\n",
        "\n",
        "    Reference: Guo,Y. et al. (2008) Using support vector machine combined with auto covariance to predict protein–protein interactions from protein sequences. Nucleic Acids Res., 36, 3025–3030.\n",
        "\n",
        "The dataset is composed of two files:\n",
        "\n",
        "> **1. Interactions**\n",
        "\n",
        "  Each entry contains two protein IDs and a binary label (1 - two given proteins interact, 0 - do not interact)\n",
        "\n",
        "\n",
        "> **2. Dictionary**\n",
        "\n",
        "  Contains protein IDs and corresponding protein sequences.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Before working with a model we have to upload and preprocess the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnZVQ7xvNABw"
      },
      "source": [
        "Let's open the two downloaded files as DataFrames using Pandas and see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-LSaXyAjCV8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Konstvv/iBio-Summer-School\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('iBio-Summer-School/Interactions.tsv', sep='\\t')\n",
        "dict_seq = pd.read_csv('iBio-Summer-School/Dictionary.tsv', sep='\\t')\n",
        "\n",
        "print(data)\n",
        "print(dict_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GElD-GnTjXx_"
      },
      "source": [
        "# Protein representations and embeddings\n",
        "\n",
        "The neural network cannot process the protein sequence represented by a string. In order to overcome this issue we have to find a way how to represent any given sequence numerically. In this class we will consider a couple of potential options\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all let's evaluate the length distribution of our sequences in the dataset. \n",
        "1. Is this dataset balanced? What are the classes proportions?\n",
        "2. Using *plt.hist* plot the sequence length distribution from *dict_seq*"
      ],
      "metadata": {
        "id": "GyPudqe63uCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enter your code here for the question 1\n",
        "\n",
        "# Enter your code here for the question 2\n",
        "# plt.hist()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "B6UyC4vgx_JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqHoIqxmPB1c"
      },
      "outputs": [],
      "source": [
        "## Introducing a couple of parameters\n",
        "\n",
        "# The sequence size for our future model\n",
        "# All the sequences will be either trimmed or extended to this size\n",
        "sequence_size = 2000  #@param {type: \"slider\", min: 100, max: 4000}\n",
        "\n",
        "# Vocabulary of aminoacids\n",
        "aminoacids = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
        "                'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEmBUiOYPB7U"
      },
      "source": [
        "## 1. One-hot embedding\n",
        "\n",
        "The features are encoded using a ‘one-of-K’ or ‘dummy’ scheme. Every letter is replaced by a vector of size *len(aminoacids)* which is filled with 0 with only one value (at position *aminoacids.index(letter)*) being 1.\n",
        "\n",
        "Please write a function **one_hot_emb** that takes a protein sequence as a string input and returns a one-hot array with the size *(sequence_size, len(aminoacids))*. If the string is longer than *sequence_size*, delete the excess (shorter - pad the rest with zeros)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs3wbCR6DdtG"
      },
      "outputs": [],
      "source": [
        "def one_hot_emb(string):\n",
        "    # A shape that has to be returned for every one-hot embedding: (sequence_size, len(aminoacids))\n",
        "    seq_tensor = np.zeros((sequence_size, len(aminoacids)), dtype=np.float16)\n",
        "    for i in range(len(string)):\n",
        "        char = string[i]\n",
        "        if char in aminoacids:\n",
        "            seq_tensor[i, aminoacids.index(char)] = 1\n",
        "        if i == sequence_size - 1:\n",
        "            break\n",
        "    return seq_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__0du9z8DUen"
      },
      "source": [
        "## 2. NLF embedding\n",
        "\n",
        "This method takes many physicochemical properties and transforms them using PCA (dimensionality reduction method) and Fisher transformation creating a small set of features that can describe an amino acid. There are 18 transformed features in total.\n",
        "\n",
        "    Reference: L. Nanni and A. Lumini, “A new encoding technique for peptide classification,” Expert Syst. Appl., vol. 38, no. 4, pp. 3185–3191, 2011.\n",
        "\n",
        "We can upload a DataFrame that contains the transrormed vectors for all the corresponding amino acids:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w10rGZQhMh-D"
      },
      "outputs": [],
      "source": [
        "nlf = pd.read_csv('https://raw.githubusercontent.com/dmnfarrell/epitopepredict/master/epitopepredict'\n",
        "                        '/mhcdata/NLF.csv', index_col=0)\n",
        "# The letter X will represent an empty position (added in case we might encounter empty spots somewhere)\n",
        "nlf['X'] = [0.0] * nlf.shape[0]\n",
        "# If we encounter Selenocysteine, we'll treat it as Cysteine\n",
        "nlf['U'] = nlf['C']\n",
        "print(nlf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw0F9w6cJP-T"
      },
      "source": [
        "Here please write a function **nlf_emb** which would be similar to *one_hot_emb* and would take a protein sequence as an input and return an array with the size *(sequence_size, 18)* where each letter will be replaced by a vector taken from the *nlf* DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6Z0ztAqJOx2"
      },
      "outputs": [],
      "source": [
        "#def nlf_emb(string):\n",
        "\n",
        "    # A shape that has to be returned for every one-hot embedding: (sequence_size, nlf.shape[0])\n",
        "    \n",
        "    ## Uncomment the first line and enter your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E61I71G5DV46"
      },
      "source": [
        "## 3. Blosum embedding\n",
        "\n",
        "BLOSUM62 is a substitution matrix that specifies the similarity of one amino acid to another by means of a score. This score reflects the frequency of substiutions found from studying protein sequence conservation in large databases of related proteins. The number 62 stands for the percentage identity at which sequences are clustered in the analysis.\n",
        "\n",
        "We can take columns of this matrix which correspond to a particular amino acid as embedding vectors. These way puts the amino acids with similar physicochemical properties closer together in the embedding space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nDUwnabT5q8"
      },
      "outputs": [],
      "source": [
        "blosum = pd.read_csv('https://raw.githubusercontent.com/dmnfarrell/epitopepredict/master/epitopepredict'\n",
        "                        '/mhcdata/blosum62.csv', index_col=0)\n",
        "# If we encounter Selenocysteine, we'll treat it as Cysteine\n",
        "blosum['U'] = blosum['C']\n",
        "# We will not encounter B, Z or * so we can delete the corresponding vectors\n",
        "blosum.drop(labels=['B', 'Z', 'X', '*'], axis=0, inplace=True)\n",
        "blosum.drop(labels=['B', 'Z', '*'], axis=1, inplace=True)\n",
        "print(blosum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0woGLAh5Pviz"
      },
      "source": [
        "Here please write a function **blosum_emb** which would be similar to *one_hot_emb* and would take a protein sequence as an input and return an array with the size *(sequence_size, blosum.shape[0])*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm4Gt2H_MaWo"
      },
      "outputs": [],
      "source": [
        "#def blosum_emb(string):\n",
        "\n",
        "    # A shape that has to be returned for every one-hot embedding: (sequence_size, blosum.shape[0])\n",
        "    \n",
        "    ## Uncomment the first line and enter your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdbmSJ6xDZKL"
      },
      "source": [
        "## 4. Other approaches\n",
        "\n",
        "There are many other different ways of how one can create a sequence embedding. The ones that may provide a good performance can be heavier and more complicated than the examples we have today. For example, the good solutions are:\n",
        "\n",
        "\n",
        "*   *Protein profiles*\n",
        "\n",
        "Profiles are used to model protein families and domains. They are built by converting multiple sequence alignments into position-specific scoring systems (PSSMs). Amino acids at each position in the alignment are scored according to the frequency with which they occur.\n",
        "\n",
        "*   Pretrained language models\n",
        "\n",
        "Various neural networks can be used to compute the embeddings while being trained on a large number of proteins. Such algorithms may use different types of information: the structural and physicochemical similarity between proteins, pairwise residue contact maps for individual proteins, etc. Usually these methods can provide more generalized view than trainable embeddings inside the predictive model due to the fact that they have been trained on large amounts of data.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buljQ1AJZyhA"
      },
      "source": [
        "Once we have written our embedding function we can use it to convert all the sequences in our protein dictionary into numpy arrays.\n",
        "Let's replace the strings in the 'Seq_string' column of the *dict_seq* DataFrame with the corresponding embeddings using one of the functions above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoTRdNPAX1g9"
      },
      "outputs": [],
      "source": [
        "## Choose the function for the embedding\n",
        "emb = 'Blosum' #@param ['Blosum', 'NLF', 'One-Hot']\n",
        "\n",
        "embs = {'Blosum' : blosum_emb, 'NLF': nlf_emb, \"One-Hot\": one_hot_emb}\n",
        "\n",
        "dict_seq['Seq_string'] = dict_seq['Seq_string'].apply(embs[emb])\n",
        "print(dict_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAm8HgnaZwQv"
      },
      "source": [
        "Then, let's replace all the protein IDs in the *data* (columns 'Seq1' and 'Seq2') with corresponding matrices from the *dict_seq*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQOQMNs5AWRG"
      },
      "outputs": [],
      "source": [
        "data = data.merge(dict_seq, left_on='Seq1', right_on='Id', how='left')\n",
        "data = data.merge(dict_seq, left_on='Seq2', right_on='Id', how='left', suffixes=('1', '2'))\n",
        "\n",
        "data = data[['Seq_string1', 'Seq_string2', 'Label']]\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2cXsm0gjoHl"
      },
      "source": [
        "# PPI prediction model\n",
        "In this section we will create a binary classification model for PPI predition. We will do that using the keras module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-lmj-WoUKmF"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "from keras import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import gc\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "# Making sure we use GPU and setting the memory gorwth\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BcbitdtdBkJ"
      },
      "source": [
        "## 1. Input - Embedding\n",
        "\n",
        "Here we need to transform the data into numpy arrays and split it into 3 parts: training, validation, and testing. We will use training dataset for the training itself. The validation data will be used to evaluate the performance of the model after each epoch and to choose the best configuration. It is also might be used to choose the optimal values of hyperparameters. The testing data will be used afterwards to make a conclusion about how well the model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uko45w84dMfA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x1 = np.asarray(data['Seq_string1'].tolist(), dtype=np.float16)\n",
        "x2 = np.asarray(data['Seq_string2'].tolist(), dtype=np.float16)\n",
        "y = np.asarray(data['Label'].tolist(), dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "## Enter your code here to split the data into train and test sets using train_test_split:\n",
        "\n",
        "\n",
        "\n",
        "# Delete the variables we do not need anymore to free some RAM in case the notebook crashes\n",
        "del data, dict_seq, x1, x2, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wp3Dk3zuTar"
      },
      "outputs": [],
      "source": [
        "input_dimensions = (sequence_size, x1_train.shape[-1])\n",
        "\n",
        "# Inputs\n",
        "seq_input1 = layers.Input(shape=input_dimensions, name='seq1')\n",
        "seq_input2 = layers.Input(shape=input_dimensions, name='seq2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XtuB9SLdDjW"
      },
      "source": [
        "## 2. Siamese module\n",
        "\n",
        "The first part of the Neural network is a so-called siamese module which process two sequences separately. It takes a keras Tensor as an input and outputs another Tensor after having done several manupulations. In this class the Siamese module will consist of three types of layers: Convolutional, MaxPool, and GRU. \n",
        "\n",
        "Here below you can find a schematic gif of 1D convolution. It shows how exactly a filter is applied to the sequence to compute the output:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://e2eml.school/images/conv1d/stride_2.gif\" />\n",
        "</p>\n",
        "\n",
        "In the simplest case the output value of conv1D layer with input size $(N, L, C_{in})$ and output size $(N, L_{out}, C_{out})$ can be calculated using the following formula:\n",
        "\n",
        "$$\n",
        "out(N_i, C_{out_j}) = bias(C_{out_j}) + ∑_{k=0}^{C_{in}-1}weight(C_{out_j}, k) ⋆ input(N_i, k)\n",
        "$$\n",
        "\n",
        "where $⋆$ is the cross-correlation operator.\n",
        "\n",
        "Write the function that processes a Tensor by using 3-4 Conv+MaxPool layers followed by a Bidirectional GRU. Make sure to initialize the layers globally and not inside the function itself, so when we call a function multiple times it would not create new instances:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaCfbitGuZ8_"
      },
      "outputs": [],
      "source": [
        "# Convolutional modules\n",
        "filters = 96\n",
        "\n",
        "conv01 = layers.Conv1D(filters, 11, padding='same', activation=\"relu\")\n",
        "mp1 = layers.MaxPooling1D(3)\n",
        "conv02 = layers.Conv1D(filters*2, 7, padding='same', activation=\"relu\")\n",
        "mp2 = layers.MaxPooling1D(3)\n",
        "conv03 = layers.Conv1D(filters*4, 3, padding='same', activation=\"relu\")\n",
        "mp3 = layers.MaxPooling1D(3)\n",
        "conv04 = layers.Conv1D(filters*2, 3, padding='same', activation=\"relu\")\n",
        "mp4 = layers.MaxPooling1D(3)\n",
        "\n",
        "gru = layers.Bidirectional(layers.CuDNNGRU(filters, return_sequences=False))\n",
        "if not gpus:\n",
        "    gru = layers.Bidirectional(layers.GRU(filters, return_sequences=False))\n",
        "\n",
        "def siamese_propagation(x):\n",
        "    #Enter your code here to create a Siamese module\n",
        "    \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG8R2nK5dIF6"
      },
      "source": [
        "## 3. Merging point\n",
        "\n",
        "The information from two sequences has to be merged at some point to produce a coherent prediction that would depend on the information from both proteins. There are many different ways to do that, but here we will consider an element-wise multiplication because a Biderectional GRU layer will produce only a single value for any given filter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zEqQsGZdKeQ"
      },
      "source": [
        "## 4. Prediction module\n",
        "\n",
        "Here we have to create a *forward* function that takes two protein embeddings as inputs, processes them with a siamese module, merges two tensors using element-wise multiplication, and then uses two Dense layers to generate final output for binary classification.\n",
        "\n",
        "*NB: Do not forget to use the sigmoid activation function in the last layer.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rnn5LukTiCpd"
      },
      "outputs": [],
      "source": [
        "def forward(left, right):\n",
        "    ## Enter your code here to create a forward propagation function\n",
        "    ## Here we have to use the following functions/classes:\n",
        "    ## siamese_propagation, layers.Dense, layers.multiply\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDj-T3ZcuIpg"
      },
      "source": [
        "## What metrics to use?\n",
        "One of the problems of training a model for PPI prediction is the fact that the datasets are often unbalanced (there are much more negative interactions that positive ones). That is why, sometimes metrics like binary accuracy or ROC AUC might be not the best choice to evaluate the model's performance. Even though here we use a balanced dataset, we will still try to use more reliable metrics.\n",
        "\n",
        "Here we will write three custom loss finctions for the accuracy, F1 score, and the Matthews correlation coefficient (MCC). These metrics provide more reliable assesments for imbalanced data.\n",
        "\n",
        "$$\n",
        "MCC = \\frac{TP × TN - FP × FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "F_{1} = \\frac{TP}{TP + \\frac{1}{2}(FP + FN)}\n",
        "$$\n",
        "\n",
        "where $TP$ - true positives, $TN$ - true negatives, $FP$ - false positives, $FN$ - false negatives\n",
        "\n",
        "The functions must take (y_true, y_pred) as two arguments, two tensors with shape (batch_size, 1) that represent the original labels and predictions respectively\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1iJjpAClG4e"
      },
      "outputs": [],
      "source": [
        "# For better compatibility one can use keras backend functions instead of numpy\n",
        "# For example, the functions that you might need to use:\n",
        "# K.sum instead of np.sum to sum up all the values in a tensor\n",
        "# K.round instead of np.round to round every element to the closest integer in a tensor\n",
        "# K.clip(x, 0, 1) to make sure that our values stay within [0, 1]\n",
        "import keras.backend as K\n",
        "\n",
        "##Enter you code here to create the 3 metrics: accuracy, f1, and mcc\n",
        "    \n",
        "def f1(y_true, y_pred):\n",
        "\n",
        "\n",
        "def mcc(y_true, y_pred):\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J8rvrYSUi55"
      },
      "source": [
        "## The loss function\n",
        "\n",
        "The classic loss function for binary classification problems is binary crossentropy. In this section we are going to write a function that will compute binary crossentropy having two tensors as an input (target values and predictions):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZVvTVWIUioK"
      },
      "outputs": [],
      "source": [
        "def binary_crossentropy(y_true, y_pred):\n",
        "    ## Enter your code here to create the loss fuction\n",
        "    ## here we can again use keras.backend (K) module\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2thXFe-1cQxi"
      },
      "source": [
        "## The model initialization and training\n",
        "\n",
        "After having prepared the architechture we can initialize the Model class and begin the learning on the preprocessed data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eNE5ZtdJk1LG"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "\n",
        "model = Model(inputs=[seq_input1, seq_input2],\n",
        "            outputs=[forward(seq_input1, seq_input2)])\n",
        "\n",
        "adam = Adam(learning_rate=1e-4, amsgrad=True, epsilon=1e-6)\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('/content/model.h5', monitor='val_mcc', mode='max')\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "model.compile(optimizer=adam, loss=binary_crossentropy, metrics=[accuracy, f1, mcc])\n",
        "\n",
        "model.fit([x1_train, x2_train], y_train, epochs=50, callbacks=[checkpoint_callback, earlystop_callback],\n",
        "          batch_size=64, verbose=1, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6EWy6fqEMoD"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW3TN_aokOKV"
      },
      "source": [
        "\n",
        "## 1. Model testing\n",
        "In this section we are going to get the predictions of the test data and compute the metrics we have used before. Moreover, we can build ROC and PR curves to estimate these metrics as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdbrVJlXrVQ8"
      },
      "outputs": [],
      "source": [
        "model_saved = tf.keras.models.load_model('/content/model.h5', custom_objects={'f1': f1, 'mcc': mcc})\n",
        "\n",
        "y_pred = model_saved.predict([x1_test, x2_test]).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0l4OhIFBkKn"
      },
      "outputs": [],
      "source": [
        "print('Test accuracy: ', accuracy(y_test, y_pred).numpy())\n",
        "print('Test F1 score: ', f1(y_test,  y_pred).numpy())\n",
        "print('Test MCC score: ', mcc(y_test, y_pred).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayDeeWhI_WGx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "PrecisionRecallDisplay.from_predictions(y_test, y_pred)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVtiXyUCAP4p"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "RocCurveDisplay.from_predictions(y_test, y_pred)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlX_OFBlw71P"
      },
      "source": [
        "# C1 - C2 - C3 classes\n",
        "\n",
        "The accuracy of PPI predictions greatly vary depending on the training and testing data. In some [state-of-the-art models](http://cb.csail.mit.edu/cb/dscript/) one can observe that the model performance becomes significantly worse when we test the model using proteins from organisms that are not related to the training set. Therefore, the performance of the model can be evaluated slightly differently: we can divide our testing set into 3 groups: C1 - pairs where both proteins were in the training data, C2 - pairs where only one of the proteins is in the training data, C3 - pairs that do not contain proteins from training data at all.\n",
        "\n",
        "In this section you need to manually create three separate testing sets for C1-C2-C3 classes and check whether the model performance is any different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Osm-pGbhvhk"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('iBio-Summer-School/Interactions.tsv', sep='\\t')\n",
        "dict_seq = pd.read_csv('iBio-Summer-School/Dictionary.tsv', sep='\\t')\n",
        "dict_seq['Seq_string'] = dict_seq['Seq_string'].apply(blosum_emb)\n",
        "\n",
        "#This function can be used to mimic the preprocessing step above\n",
        "def preprocess_dataset(data):\n",
        "  data = data.merge(dict_seq, left_on='Seq1', right_on='Id', how='left')\n",
        "  data = data.merge(dict_seq, left_on='Seq2', right_on='Id', how='left', suffixes=('1', '2'))\n",
        "\n",
        "  data = data[['Seq_string1', 'Seq_string2', 'Label']]\n",
        "\n",
        "  x1 = np.asarray(data['Seq_string1'].tolist(), dtype=np.float16)\n",
        "  x2 = np.asarray(data['Seq_string2'].tolist(), dtype=np.float16)\n",
        "  y = np.asarray(data['Label'].tolist(), dtype=np.float32)\n",
        "\n",
        "  return x1, x2, y\n",
        "\n",
        "## Enter your code here to create a new training set (x1_train, x2_train, y_train)\n",
        "## and 3 new test sets (x1_C1, x2_C1, y_C1), (x1_C2, x2_C2, y_C2), (x1_C3, x2_C3, y_C3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM8xgleRdZxr"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "\n",
        "model = Model(inputs=[seq_input1, seq_input2],\n",
        "            outputs=[forward(seq_input1, seq_input2)])\n",
        "\n",
        "adam = Adam(learning_rate=1e-4, amsgrad=True, epsilon=1e-6)\n",
        "\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=[accuracy, f1, mcc])\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('/content/model_C1_C2_C3.h5', monitor=\"val_loss\")\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "model.fit([x1_train, x2_train], y_train, epochs=50, callbacks=[checkpoint_callback, earlystop_callback],\n",
        "          batch_size=64, verbose=2, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dhum9V5Qe4c1"
      },
      "outputs": [],
      "source": [
        "model_saved = tf.keras.models.load_model('/content/model_C1_C2_C3.h5', custom_objects={'f1': f1, 'mcc': mcc})\n",
        "\n",
        "y_pred_C1 = model_saved.predict([x1_C1, x2_C1]).flatten()\n",
        "y_pred_C2 = model_saved.predict([x1_C2, x2_C2]).flatten()\n",
        "y_pred_C3 = model_saved.predict([x1_C3, x2_C3]).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSZmZoMPfGgR"
      },
      "outputs": [],
      "source": [
        "## Enter your code here to test the model on three different test sets using previously defined metrics (e.g., accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "iBio Summer school PPI",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}